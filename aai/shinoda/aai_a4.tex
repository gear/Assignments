%
% ART.T548 - Advanced artificial intelligence 
% Author: Hoang Nguyen
%
\documentclass[12pt,twoside]{article}

\usepackage{amsmath, amssymb}
\usepackage{graphicx}
\usepackage{dsfont}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{subcaption}

\input{macros}

\setlength{\oddsidemargin}{0pt}
\setlength{\evensidemargin}{0pt}
\setlength{\textwidth}{6.5in}
\setlength{\topmargin}{0in}
\setlength{\textheight}{8.5in}

% Fill these in!
\newcommand{\theproblemsetnum}{3}
\newcommand{\releasedate}{Thursday, July 31, 2016}
\newcommand{\partaduedate}{Monday, August 8, 2016}
\newcommand{\tabUnit}{3ex}
\newcommand{\tabT}{\hspace*{\tabUnit}}

\makeatletter
\newcommand*{\indep}{%
\mathbin{%
\mathpalette{\@indep}{}%
}%
}
\newcommand*{\nindep}{%
\mathbin{%                   % The final symbol is a binary math operator
\mathpalette{\@indep}{\not}% \mathpalette helps for the adaptation
% of the symbol to the different math styles.
}%
}
\newcommand*{\@indep}[2]{%
% #1: math style
% #2: empty or \not
\sbox0{$#1\perp\m@th$}%        box 0 contains \perp symbol
\sbox2{$#1=$}%                 box 2 for the height of =
\sbox4{$#1\vcenter{}$}%        box 4 for the height of the math axis
\rlap{\copy0}%                 first \perp
\dimen@=\dimexpr\ht2-\ht4-.2pt\relax
% The equals symbol is centered around the math axis.
% The following equations are used to calculate the
% right shift of the second \perp:
% [1] ht(equals) - ht(math_axis) = line_width + 0.5 gap
% [2] right_shift(second_perp) = line_width + gap
% The line width is approximated by the default line width of 0.4pt
\kern\dimen@
{#2}%
% {\not} in case of \nindep;
% the braces convert the relational symbol \not to an ordinary
% math object without additional horizontal spacing.
\kern\dimen@
\copy0 %                       second \perp
} 
\makeatother

\begin{document}

\handout{Quiz 3 - Lecture 14 (Prof. Shinoda)}{\releasedate}

\newif\ifsolution
\solutiontrue
\newcommand{\solution}{\textbf{Solution:}}

\noindent

\begin{enumerate}
  \item Prove that $p(\mathbf{x}) = \sum_{k=1}^{K} \pi_k \mathcal{N}(\mathbf{x}|\boldsymbol \mu_k,\boldsymbol \Sigma_k)$
  \item Discuss the future prospect of deep learning and its related techniques.
\end{enumerate}

\setlength{\parindent}{0pt}

\hrulefill

\textbf{Collaborators:}
%%% COLLABORATORS START %%%
None.
%%% COLLABORATORS END %%%

\begin{exercises}

\problem \textbf{Prove that} $p(\mathbf{x}) = \sum_{k=1}^{K} \pi_k \mathcal{N}(\mathbf{x}|\boldsymbol \mu_k,\boldsymbol \Sigma_k)$

\ifsolution \solution{}
%%% Ex1 SOLUTION START %%%
By definition, $\mathbf{z}$ is one-hot encoding representation, we have:

\begin{equation*}
  \begin{aligned}
    p(\mathbf{z}) & = \prod^{K}_{k=1} \pi_k^{z_k} \\
    p(\mathbf{x}|\mathbf{z}) & = \prod^{K}_{k=1} \mathcal{N} (\mathbf{x}|\boldsymbol \mu_k, \boldsymbol \Sigma_k)^{z_k}
  \end{aligned}
\end{equation*}

By the product rule, we have the join probability of $\mathbf{x}$ and $\mathbf{z}$ as follow:
$$ p(\mathbf{x}, \mathbf{z}) = p(\mathbf{x}|\mathbf{z}) p(\mathbf{z}) $$

Using the sum product to compute the marginal $p(\mathbf{x})$:
\begin{equation*}
  \begin{aligned}
    p(\mathbf{x}) & = \sum_{\mathbf{z}} p(\mathbf{x}|\mathbf{z}) p(\mathbf{z})\\
                  & = \sum_{\mathbf{z}} \prod^{K}_{k=1} \mathcal{N} (\mathbf{x}|\boldsymbol \mu_k, \boldsymbol \Sigma_k)^{z_k} \pi_k^{z_k}
                  & = \sum_{j=1}^{K} \prod^{K}_{k=1} (\pi_k \mathcal{N}(\mathbf{x}|\boldsymbol \mu_k, \boldsymbol \Sigma_k)^{\delta_{jk}},
  \end{aligned}
\end{equation*}
where $\delta_{jk}$ is the Kronecker delta. Simply rewrite the product keeping not-1 values, we have the desired result:

$$ p(\mathbf{x}) = \sum_{k=1}^{K} \pi_k \mathcal{N}(\mathbf{x}|\boldsymbol \mu_k,\boldsymbol \Sigma_k) $$

%%% Ex1 SOLUTION END %%%
\fi

\problem \textbf{Discuss the future prospect of deep learning and related techniques.}

\ifsolution \solution{}
%%% PROBLEM 2(a) SOLUTION START %%
Recently, machine learning and especially deep learning technique have been
employed into everyday life. Deep learning technique originates from the
effort to model human's neural network. However, our best current models only
can mimic a small fraction of biological brain work functions. The challenges
include: finding an effective activation function for neurons, automate training
data acquisition, and multi-task machine. Currently, our best models still use
very simple activation function artificial neuron to keep the back-propagation
computation cost tractable. Therefore, the one of the challenges for future deep 
learing and cognitive science researchers is to find the ``holy grail'' activation
function. On another matter, current models cannot acquire knowledge by themselves,
but they must rely on man-made inputs and data structure. In the world, few Machine 
Learning groups are working on the problem of choosing training samples that focus 
on diversity by using submodularity set functions. Another demand for machine 
learning system is the capacity for performing multiple task and automatic logical 
reasoning.  Finally, training a deep learning system requires high performance 
computing as well as long training time. The training time of deep learning system
doesn't scale with the data produced everyday. As a result, many machine-based system
still employ traditional learning technique such as decision tree, random forest, or
SVM. One possible solution for this problem in near future is to employ quantum 
computing in the deep network training process. In 2016, D-Waves has introduced 
D2X with 1000 q-bits. D2X has produced result for graph coloring problem with more
than 600 vertices in an instance. Furthermore, Google and D-Waves have
been working together for quantum machine learning project. Their intial results on
car recognition for self-driving car outperforms current graphic card based optimization.
I believe deep learning techniques will soon leave the research labs and become the
most essential part of mankind.
%%% PROBLEM 2(a) SOLUTION END %%%
\fi


\end{exercises}

\end{document}
