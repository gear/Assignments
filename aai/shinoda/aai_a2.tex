%
% ART.T548 - Advanced artificial intelligence 
% Author: Hoang Nguyen
%
\documentclass[12pt,twoside]{article}

\usepackage{amsmath, amssymb}
\usepackage{graphicx}
\usepackage{dsfont}

\input{macros}

\setlength{\oddsidemargin}{0pt}
\setlength{\evensidemargin}{0pt}
\setlength{\textwidth}{6.5in}
\setlength{\topmargin}{0in}
\setlength{\textheight}{8.5in}

% Fill these in!
\newcommand{\theproblemsetnum}{1}
\newcommand{\releasedate}{Monday, July 25, 2016}
\newcommand{\partaduedate}{Thursday, July 28}
\newcommand{\tabUnit}{3ex}
\newcommand{\tabT}{\hspace*{\tabUnit}}

\begin{document}

\handout{Quiz 1 - Lecture 12 (Prof. Shinoda)}{\releasedate}

\newif\ifsolution
\solutiontrue
\newcommand{\solution}{\textbf{Solution:}}

\begin{enumerate}
  \setlength{\parskip}{0pt}
  \item Prove the second formula in Slide 25.
  \item Prove the second formula in Slide 36.
  \item We think about an 1-dimension with mean $\mu$, variance $\sigma^2$. Prove that the distribution which maximize the entropy is a Gaussian distribution.
\end{enumerate}

\setlength{\parindent}{0pt}

\medskip

\hrulefill

\textbf{Collaborators:}
%%% COLLABORATORS START %%%
None.
%%% COLLABORATORS END %%%

\begin{exercises}

\problem \textbf{Prove the second formula in Slide 25}

Consider the expectations of the variation with respect to the data
set values, which comes from a Gaussian distribution with parameter
$\mu$ and $\sigma^2$. Prove the following formula: 
\hspace*{-1em}
$$\mathds{E}\lbrack\sigma_{\scalebox{0.7}{ML}}^2\rbrack = \left ( \frac{N-1}{N} \right ) \sigma^2 $$

\ifsolution \solution{}
%%% Ex1 SOLUTION START %%%
\begin{equation}
  \begin{aligned}
    \mathds{E}\lbrack\sigma_{\scalebox{0.7}{ML}}^2\rbrack 
      &= \mathds{E}\left\lbrack\frac{1}{N}\sum_{n=1}^{N} (x_n - \mu_{\scalebox{0.7}{ML}})^2\right\rbrack
      = \mathds{E}\left\lbrack\frac{1}{N}\sum_{n=1}^{N}\left(x_n-\frac{1}{N}\sum_{n=1}^{N}x_n\right)^2\right\rbrack\\
      &= \frac{1}{N}\sum_{n=1}^{N}\mathds{E}\left\lbrack\left(x_n-\frac{1}{N}\sum_{n=1}^{N}x_n\right)^2\right\rbrack\\ 
      &= \frac{1}{N}\sum_{n=1}^{N}\mathds{E}\left\lbrack x_n^2 - \frac{2}{N}x_n\sum_{m=1}^{N}x_m+\frac{1}{N^2}\sum_{m=1}^{N}\sum_{k=1}^{N}x_m x_k\right\rbrack \\
      &= \frac{1}{N}\sum_{n=1}^{N}\left(\mathds{E}\left\lbrack x_n^2\right\rbrack - \frac{2}{N}\mathds{E}\left\lbrack x_n\sum_{m=1}^{N}x_m\right\rbrack+\frac{1}{N^2}\mathds{E}\left\lbrack\sum_{m=1}^{N}\sum_{k=1}^{N}x_m x_k\right\rbrack\right) \\
  \end{aligned}
\end{equation}

We have these following equalities:
\begin{equation}
  \begin{aligned}
    & \mathds{E}\lbrack x_n^2 \rbrack = \mu^2 + \sigma^2 \\
    & \mathds{E}\left\lbrack x_n\sum_{m=1}^{N}x_m\right\rbrack = 
        \mathds{E}\left\lbrack x_n^2 \right\rbrack + \sum_{m = 1}^{N-1}\mathds{E}\lbrack x_n \rbrack \mathds{E} \lbrack x_m \rbrack = N\mu^2 + \sigma^2 \\
    & \mathds{E}\left\lbrack\sum_{m=1}^{N}\sum_{k=1}^{N}x_m x_k\right\rbrack\ = N \mathds{E} \lbrack x_n^2 \rbrack + 2 \sum_{m=1}^{N-1}\sum_{k=m+1}^{N} \mathds{E} \lbrack x_m \rbrack \mathds{E} \lbrack x_k \rbrack = N^2\mu^2 + N\sigma^2\\
  \end{aligned}
\end{equation}

Replace results from (2) to equation (1), we have:

\begin{equation}
  \begin{aligned}
    \mathds{E}\lbrack\sigma_{\scalebox{0.7}{ML}}^2\rbrack 
      &= \frac{1}{N}\sum_{n=1}^{N}\left( \mu^2 + \sigma^2 - 2\left(\mu^2 + \frac{1}{N}\sigma^2\right) + \mu^2 + \frac{1}{N}\sigma^2 \right) \\
      &= \left( \frac{N-1}{N} \right) \sigma^2\ \  \blacksquare
  \end{aligned}
\end{equation}
%%% Ex1 SOLUTION END %%%
\fi

\problem \textbf{Prove the second formula in Slide 36}

Entropy is maximized when: $\forall i : p_i = \frac{1}{M}$

\ifsolution \solution{}
%%% PROBLEM 2(a) SOLUTION START %%%
The entropy of an M-state discrete variable $x$ can be written as:
$$ \mbox{H}(x) = -\sum_{i=1}^{M}p(x_i)\ln p(x_i) = \sum_{i=1}^{M} p(x_i) \ln \frac{1}{p(x_i)}$$
Since $\ln(x)$ is concave, therefore we can apply the reverse Jensen's inequality:
$$ f\left( \sum_{i=1}^{M} \lambda_i x_i \right) \geq \sum_{i=1}^{M} \lambda_i f(x_i) $$
From the above inequality, we have:
$$ \mbox{H}(x) \leq \ln \left( \sum_{i=1}^{M} p(x_i) \frac{1}{p(x_i)} \right) = \ln M$$
The equal sign can be achieved with:
$$ p_i = \frac{1}{M} $$
Hence, we have proved that the entropy is bounded by $\ln M$ and it happens when $p_i = \frac{1}{M}$.
%%% PROBLEM 2(a) SOLUTION END %%%
\fi

\problem \textbf{Prove entropy maxima for a distribution}

Prove that entropy of a Gaussian distribution with mean $\mu$ and variance $\sigma^2$ is the upper bound for entrpy of some distribution with same mean and variance.

\ifsolution \solution{}
Consider a distribution $p(x)$ with fixed mean $\mu$ and variance $\sigma^2$. The entropy of the normal distribution with same entropy and variance is:
$$ \mbox{H}(\mathcal{N}(x;\mu,\sigma^2)) = \frac{1}{2}\ln (2\pi e \sigma^2) $$
Use the upper bound above, we have:
\begin{equation*}
  \begin{aligned}
    \mbox{H}(p) & \leq - \int p(x) \ln \mathcal{N}(x;\mu,\sigma^2)dx \\
    & \leq - \int p(x) \ln \left(\frac{1}{\sqrt{2\pi \sigma^2}}\exp(-\frac{(x-\mu)^2}{2\sigma^2})\right) \\
    & \leq \frac{1}{2\sigma^2} \int p(x)(x-\mu)^2 dx  + \frac{1}{2}\ln (2\pi \sigma^2) \\
    & \leq \frac{1}{2} \ln (2 \pi e \sigma^2) = \mbox{H}(\mathcal{N}(x;\mu,\sigma^2)) 
  \end{aligned}
\end{equation*}
Therefore, the entropy upper bound a any distribution $p(x)$ with fixed $\mu$ and $\sigma^2$ is the entropy of the Gaussian distribution with same $\mu$ and $\sigma^2$.
\fi


\end{exercises}

\end{document}
